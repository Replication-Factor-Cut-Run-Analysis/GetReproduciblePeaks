configfile: "config/config.yml"
import pandas as pd
from snakemake.io import glob_wildcards, expand
# set unique ID to append to files
import random
random.seed(10)
ID=str(random.random())[2,8]

##################################################################
##                    Define input functions                    ##
##################################################################

# this reads the CSV file and sets an index using the values in the "sample" column.
samples_table = pd.read_csv(config["samples_csv"]).set_index("sample", drop=False)
# convert all values in dataframe to strings
samples_table = samples_table.applymap(str)
# make a samples list
samples_lst = samples_table['sample'].to_list()
# make sets list
sets_lst = set(samples_table['set'].to_list())
# treatmentBam filename input function definition set to Python dictionary
def Bam_dict_from_sample(wildcards):
  return {
    "txBam": samples_table.loc[wildcards.sample, "treatmentBam"],
    "inBam": samples_table.loc[wildcards.sample, "inputBam"]
  }
# function that returns dict of mapped Read Counts filenames/paths joined (ie space-separated) for a set of interest
def CountFiles_dict_from_set(setOfInterest):
  sample_lst = samples_table.loc[samples_table["set"] == str(setOfInterest),"sample"].to_list()
  tx_counts_files_list = expand('results/mappedReadCounts/{sample}_treated_mappedReadCounts.txt', sample = sample_lst)
  in_counts_files_list = expand('results/mappedReadCounts/{sample}_input_mappedReadCounts.txt', sample = sample_lst)
  tx_string = ' '.join([str(item) for item in tx_counts_files_list])
  in_string = ' '.join([str(item) for item in in_counts_files_list])
  return [tx_string, in_string]
# functions that return dicts of bam filenames/paths joined (ie space-separated) for each sample
def Tx_BamFiles_dict_from_samples(sample):
  bam_lst = set(samples_table.loc[samples_table["sample"] == str(sample),"treatmentBam"].to_list())
  return ' '.join([str(item) for item in bam_lst])
def In_BamFiles_dict_from_samples(sample):
  bam_lst = set(samples_table.loc[samples_table["sample"] == str(sample),"inputBam"].to_list())
  return ' '.join([str(item) for item in bam_lst])


##################################################################
##                           rules                              ##
##################################################################

# to run snakemake without explicitly requesting any output files on the command line, we must request output files in the first rule. Therefore we include this otherwise useless rule here  
rule all:
    input:
        expand("results/macs2_normalPeaks/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_peaks.narrowPeak", sample = samples_lst),
        "results/eulerPlot/" + str(config["experimentName"]) + "_" + str(config['macs2_minimum_FDR_cutoff']) + "_eulerPlot.rds",
        "results/eulerPlot/" + str(config["experimentName"]) + "_" + str(config['macs2_minimum_FDR_cutoff']) + "_eulerPlot.pdf",
        expand("results/backgroundNormalizedBigwigs/{sample}_bkgrndNorm.bw", sample = set(samples_lst)),
        "results/heatPlotOfReadsAcrossPeakSummits/" + str(config["experimentName"]) + "_" + str(config['macs2_minimum_FDR_cutoff']) + "_" + str(config['minNumberOfSampleOverlaps']) + "_heatPlotAcrossPeakSummits.rds",
        "results/heatPlotOfReadsAcrossPeakSummits/" + str(config["experimentName"]) + "_" + str(config['macs2_minimum_FDR_cutoff']) + "_" + str(config['minNumberOfSampleOverlaps']) + "_heatPlotAcrossPeakSummits.pdf",
        
# merge technical replicates
rule merge_technical_replicates:
    params:
        tx_technical_replicates = lambda wildcards: Tx_BamFiles_dict_from_samples(wildcards.sample),
        in_technical_replicates = lambda wildcards: In_BamFiles_dict_from_samples(wildcards.sample),
    output:
        tx_merged_bam = "results/mergedTechnicalReplicates/{sample}_tx.bam",
        in_merged_bam = "results/mergedTechnicalReplicates/{sample}_in.bam",
    envmodules: 
        config["samtools"],
    shell:
        """
        samtools merge -@ 8 {output.tx_merged_bam} {params.tx_technical_replicates}
        samtools index -@ 8 {output.tx_merged_bam}
        samtools merge -@ 8 {output.in_merged_bam} {params.in_technical_replicates}
        samtools index -@ 8 {output.in_merged_bam}
        """
        
# call narrow peaks with macs2
rule call_narrow_peaks_with_macs2:
    input:
        tx_merged_bam = "results/mergedTechnicalReplicates/{sample}_tx.bam",
        in_merged_bam = "results/mergedTechnicalReplicates/{sample}_in.bam",
    output:
        "results/macs2_normalPeaks/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_peaks.narrowPeak"
    params:
        effective_genome_size=config["effective_genome_size"],
        minimum_FDR_cutoff=str(config["macs2_minimum_FDR_cutoff"]),
        sample_name="{sample}"
    envmodules:
        config["macs2"]
    shell:
        """
        macs2 callpeak -t {input.tx_merged_bam} -c {input.in_merged_bam} -f BAMPE -g {params.effective_genome_size} -n {params.sample_name}_{params.minimum_FDR_cutoff} -q {params.minimum_FDR_cutoff} --outdir results/macs2_normalPeaks/
        """

# write read counts for each bam to a file
rule write_read_counts:
    input:
        tx_merged_bam = "results/mergedTechnicalReplicates/{sample}_tx.bam",
        in_merged_bam = "results/mergedTechnicalReplicates/{sample}_in.bam",
    output:
        treatmentCountFile="results/mappedReadCounts/{sample}_treated_mappedReadCounts.txt",
        inputCountFile="results/mappedReadCounts/{sample}_input_mappedReadCounts.txt",
    envmodules:
        config["samtools"]
    shell:
        """
        samtools idxstats {input.tx_merged_bam} | awk -F '\t' '{{s+=$3}}END{{print s}}' > {output.treatmentCountFile}
        samtools idxstats {input.in_merged_bam} | awk -F '\t' '{{s+=$3}}END{{print s}}' > {output.inputCountFile}
        """
 
# downsample read counts to minimum
rule downsample_min_read_counts:
    input:
        tx_merged_bam = "results/mergedTechnicalReplicates/{sample}_tx.bam",
        in_merged_bam = "results/mergedTechnicalReplicates/{sample}_in.bam",
        tx_counts=expand("results/mappedReadCounts/{sample}_treated_mappedReadCounts.txt", sample = samples_lst),
        in_counts=expand("results/mappedReadCounts/{sample}_input_mappedReadCounts.txt", sample = samples_lst),
    output:
        tx_downsampled_bam = "results/downSampledBams/{sample}_tx.bam",
        in_downsampled_bam = "results/downSampledBams/{sample}_in.bam",
    envmodules:
        config["samtools"]
    shell:
        """
        tx_min=$(cat {input.tx_counts} | sort -n | head -1)
        in_min=$(cat {input.in_counts} | sort -n | head -1)
        
        downSampleBam () {{
          fraction=$(samtools idxstats $2 | cut -f3 | awk -v ct=$1 'BEGIN {{total=0}} {{total += $1}} END {{print ct/total}}')
          if $fraction < 1
          then
          samtools view -@ 8 -b -s $fraction $2 > $3
          else
          cp $2 $3
          fi     
        }}
        
        downSampleBam ${{tx_min}} {input.tx_merged_bam} {output.tx_downsampled_bam}
        downSampleBam ${{in_min}} {input.in_merged_bam} {output.in_downsampled_bam}
        """

# merge downsampled samples
rule merged_downsampled_bams:
    input:
        expand("results/downSampledBams/{sample}_tx.bam",sample = samples_lst),
        expand("results/downSampledBams/{sample}_in.bam",sample = samples_lst),
    output:
        tx_merged_downsampled_bam = "results/mergedDownSampledBams/{sample}_tx.bam",
        in_merged_downsampled_bam = "results/mergedDownSampledBams/{sample}_in.bam",
    params:
        tx_bams=' '.join(expand("results/downSampledBams/{sample}_tx.bam",sample = set(samples_lst))),
        in_bams=' '.join(expand("results/downSampledBams/{sample}_in.bam",sample = set(samples_lst))),
    envmodules:
        config["samtools"]
    shell:
        """
        samtools merge -@ 8 {output.tx_merged_downsampled_bam} {params.tx_bams}
        samtools index -@ 8 {output.tx_merged_downsampled_bam}
        samtools merge -@ 8 {output.in_merged_downsampled_bam} {params.in_bams}
        samtools index -@ 8 {output.in_merged_downsampled_bam}
        """

# call narrow peaks with macs2 on merged bams
rule call_narrow_peaks_with_macs2_on_merged:
    input:
        tx_merged_downsampled_bam = "results/mergedDownSampledBams/{sample}_tx.bam",
        in_merged_downsampled_bam = "results/mergedDownSampledBams/{sample}_in.bam",
    output:
        "results/macs2_normalPeaks_merged/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_peaks.narrowPeak",
        "results/macs2_normalPeaks_merged/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_summits.bed",
    params:
        effective_genome_size=config["effective_genome_size"],
        minimum_FDR_cutoff=str(config["macs2_minimum_FDR_cutoff"]),
        sample_name="{sample}"
    envmodules:
        config["macs2"]
    shell:
        """
        macs2 callpeak -t {input.tx_merged_downsampled_bam} -c {input.in_merged_downsampled_bam} -f BAMPE -g {params.effective_genome_size} -n {params.sample_name}_{params.minimum_FDR_cutoff} -q {params.minimum_FDR_cutoff} --outdir results/macs2_normalPeaks_merged/
        """
        
# get overlaps between peaks of merged sample and individual samples
rule report_peaks_overlapping_with_merged:
    input:
        "results/macs2_normalPeaks_merged/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_peaks.narrowPeak",
    output:
        "results/mergedPeakOverlaps/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_peaks.rds",
    params:
        ','.join(expand("results/macs2_normalPeaks/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_peaks.narrowPeak", sample = set(samples_lst))),
    envmodules:
        config["R"],
        config["Bioconductor"],
    script:
        "scripts/reportOverlappingPeaks.R"

# make euler plot
rule make_euler_plot_of_overlaps_with_merged:
    input:
        "results/mergedPeakOverlaps/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_peaks.rds",
    output:
        "results/eulerPlot/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_eulerPlot.rds",
        "results/eulerPlot/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_eulerPlot.pdf",
    params:
        config["EulerFontSize"],
        config["EulerColors"],
        config["EulerWidth"],
        config["EulerHeight"],
    envmodules:
        config["R"],
        config["Bioconductor"],
    script:
        "scripts/makeEulerOfOverlappingPeaks.R"

# make bed file of reproducible peaks
rule make_bed_of_reproducible_peaks:
    input:
        "results/mergedPeakOverlaps/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_peaks.rds",
    output:
        "results/reproduciblePeaksBed/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_" + str(config['minNumberOfSampleOverlaps']) + "_reproduciblePeaks.bed",
    params:
        config["minNumberOfSampleOverlaps"],
    envmodules:
        config["R"],
        config["Bioconductor"],
    script:
        "scripts/make_bed_of_reproducible_peaks.R"
        
# make bed file of reproducible summits
rule make_bed_of_reproducible_summits:
    input:
        peaks = "results/reproduciblePeaksBed/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_" + str(config['minNumberOfSampleOverlaps']) + "_reproduciblePeaks.bed",
        summits = "results/macs2_normalPeaks_merged/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_summits.bed",
    output:
        "results/reproduciblePeaksSummits/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_" + str(config['minNumberOfSampleOverlaps']) + "reproducibleSummits.bed",
    params:
        config["minNumberOfSampleOverlaps"],
    envmodules:
        config["bedops"],
    shell:
        """
        sort-bed {input.summits} > {input.summits}_sorted.bed
        sort-bed {input.peaks} > {input.peaks}_sorted.bed
        bedops --element-of 1 {input.summits}_sorted.bed {input.peaks}_sorted.bed > {output}
        """

## This rule should be modified in the future to accomodate scale factors calculated from spike-ins
# make normalized bigwigs using bamcompare
rule make_normalized_bigwigs_with_bamcompare:
    input:
        tx_merged_bam = "results/mergedTechnicalReplicates/{sample}_tx.bam",
        in_merged_bam = "results/mergedTechnicalReplicates/{sample}_in.bam",
    output:
        "results/backgroundNormalizedBigwigs/{sample}_bkgrndNorm.bw",
    params:
        sfm=config["scaleFactorsMethod"],
        nmu=config["normalizeUsing"],
        bco=config["bamcompareOperation"],
        bns=config["binSize"],
        bln=config["blackListFileName"],
        egs=config["effective_genome_size"],
        nop=config["numberOfProcessors"],
        mfl=config["minFragmentLength"],
        mxl=config["maxFragmentLength"],
        mmq=config["minMappingQuality"],
        sml=config["smoothLength"],
    envmodules:
        config["deeptools"],
    shell:
        """
        bamCompare --smoothLength {params.sml} --minFragmentLength {params.mfl} --maxFragmentLength {params.mxl} --minMappingQuality {params.mmq} --numberOfProcessors {params.nop} --effectiveGenomeSize {params.egs} --scaleFactorsMethod {params.sfm} --normalizeUsing {params.nmu} --operation {params.bco} --binSize {params.bns} --blackListFileName {params.bln} --centerReads --numberOfProcessors {params.nop} -b1 {input.tx_merged_bam} -b2 {input.in_merged_bam} -o {output}
        """

# make coverage matrix across reproducible peak summits
rule make_coverage_matrix_across_peaks:
    input:
        summits = "results/reproduciblePeaksSummits/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_" + str(config['minNumberOfSampleOverlaps']) + "reproducibleSummits.bed",
    output:
        mx = "results/matrixOfReadsAcrossPeakSummits/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_" + str(config['minNumberOfSampleOverlaps']) + "_readCountsAcrossSummits.tab",
        gz = "results/matrixOfReadsAcrossPeakSummits/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_" + str(config['minNumberOfSampleOverlaps']) + "_readCountsAcrossSummits.gz",
    params:
        readsBigWigs = ' '.join(expand("results/backgroundNormalizedBigwigs/{sample}_bkgrndNorm.bw",sample = set(samples_lst))),
        brl=config["beforeRegionStartLength"],
        arl=config["afterRegionStartLength"],
        bns=config["binSize"],
        bln=config["blackListFileName"],
        nop=config["numberOfProcessors"],
    envmodules:
        config["deeptools"],
    shell:
        """
        computeMatrix reference-point --outFileName {output.gz} -a {params.arl} -b {params.brl} --numberOfProcessors {params.nop} --smartLabels --outFileNameMatrix {output.mx} -S {params.readsBigWigs} -R {input.summits}
        """

# make heat plots of coverage over reproducible peaks
rule make_heatplot_of_reproducible_peaks:
    input:
        mx = "results/matrixOfReadsAcrossPeakSummits/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_" + str(config['minNumberOfSampleOverlaps']) + "_readCountsAcrossSummits.tab",
    output:
        "results/heatPlotOfReadsAcrossPeakSummits/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_" + str(config['minNumberOfSampleOverlaps']) + "_heatPlotAcrossPeakSummits.rds",
        "results/heatPlotOfReadsAcrossPeakSummits/{sample}_" + str(config['macs2_minimum_FDR_cutoff']) + "_" + str(config['minNumberOfSampleOverlaps']) + "_heatPlotAcrossPeakSummits.pdf",
    params:
        config["colorsForHeatPlot"],
        config["heatplotBreaks"],
        config["pdfWidth"],
        config["pdfHeight"],
    envmodules:
        config["R"],
        config["Bioconductor"],
    script:
        "scripts/makeHeatPlot.R"
  
